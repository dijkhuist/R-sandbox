print("Algorithm converged")
print(paste("Final gradient norm is",sqrt(sum(grad^2))))
values<-list("coef" = t(theta), "l2loss" = l2loss)
return(values)
}
gdec.eta1 = gradientR(y = y, X = data.frame(x1,x2,x3, x4,x5), eta = 100, iters = 1000)
gradientDesc <- function(x, y, learn_rate, conv_threshold, n, max_iter) {
plot(x, y, col = "blue", pch = 20)
m <- runif(1, 0, 1)
c <- runif(1, 0, 1)
yhat <- m * x + c
MSE <- sum((y - yhat) ^ 2) / n
converged = F
iterations = 0
while(converged == F) {
## Implement the gradient descent algorithm
m_new <- m - learn_rate * ((1 / n) * (sum((yhat - y) * x)))
c_new <- c - learn_rate * ((1 / n) * (sum(yhat - y)))
m <- m_new
c <- c_new
yhat <- m * x + c
MSE_new <- sum((y - yhat) ^ 2) / n
if(MSE - MSE_new <= conv_threshold) {
abline(c, m)
converged = T
return(paste("Optimal intercept:", c, "Optimal slope:", m))
}
iterations = iterations + 1
if(iterations > max_iter) {
abline(c, m)
converged = T
return(paste("Optimal intercept:", c, "Optimal slope:", m))
}
}
}
# Run the function
gradientDesc(disp, mpg, 0.0000293, 0.001, 32, 2500000)
attach(mtcars)
gradientDesc(disp, mpg, 0.0000293, 0.001, 32, 2500000)
row_m
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*(t(x)%*%(x%*%theta - y))
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
# for a R function to return two values, we need to use a list to store them:
results<-list(intercept_new,theta_new)
return(results)
}
gradientdescent_osl<-function (row,coef,learning_rate){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[-1,]
intercept=coef_m[1]
gradDescent (y, x,intercept,coef,prev_yat,0.0001)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
predict_osl<-function(row,coef){
row_m=as.matrix(row)
#print(row_m[3])
#print(row_m)
coef_m=as.matrix(coef)
n=ncol(row)-1
yhat_intercept = coef_m[1]
#print(yhat_intercept)
y_pred=0
for (i in 1:n){
#print(row_m[i+1])
#print(coef_m[i+1])
y_pred = y_pred + coef_m[i + 1] * row_m[i+1]
#print(y_pred)
}
#convert the result into a logit function
yhat_return=plogis(yhat_intercept+y_pred)
return(yhat_return)
}
y_pred=predict_osl(train_titanic_3,coef)
print(y_pred)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[-1,]
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef,prev_yhat,alpha)
print(result)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef)
glm_model_pred
glm_model<- speedglm(y~x1+x2,train_titanic_1,family = binomial(logit))
summary(glm_model)
glm_model_pred=predict(glm_model,newdata=train_titanic_3,type="response")
yhat <- ifelse(glm_model_pred > 0.5,1,0)
coef=(coef(glm_model))
y_pred=predict_osl(train_titanic_3,coef)
print(y_pred)
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[-1,]
print(x)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[-1,]
print(y)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[-2,]
print(x)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[,-1]
print(x)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[,-1]
print(x)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[,-1]
print(x)
print(coef_m)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
coef_m=coef_m[,-1]
y=row_m[1]
x=row_m[,-1]
print(x)
print(coef_m)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
coef_m=coef_m[-1,]
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
coef_m=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
print(x)
print(coef_m)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
# for a R function to return two values, we need to use a list to store them:
results<-list(intercept_new,theta_new)
return(results)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
coef_m=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
print(x)
print(coef_m)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
coef_m=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
print(x)
print(coef_m)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
print(result)
print(coef)
}
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
print(intercept)
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
print(intercept_new)
# for a R function to return two values, we need to use a list to store them:
results<-list(intercept_new,theta_new)
return(results)
}
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
coef_m=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
print(x)
print(coef_m)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
coef_m=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
intercept=coef_m[1]
coef_x=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
result=gradDescent (y, x,intercept,coef_x,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
# for a R function to return two values, we need to use a list to store them:
results<-list(intercept_new,theta_new)
return(results)
}
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
intercept=coef_m[1]
coef_x=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
result=gradDescent (y, x,intercept,coef_x,prev_yhat,alpha)
print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
coef_new=gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef_new)
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
results_c=c(intercept_new,theta_new)
print(results_c)
# for a R function to return two values, we need to use a list to store them:
results<-list(intercept_new,theta_new)
return(results)
}
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
intercept=coef_m[1]
coef_x=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
result=gradDescent (y, x,intercept,coef_x,prev_yhat,alpha)
#print(result)
#print(coef)
}
coef_new=gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef_new)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
intercept=coef_m[1]
coef_x=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
results=gradDescent (y, x,intercept,coef_x,prev_yhat,alpha)
return(results)
}
coef_new=gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef_new)
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
results=c(intercept_new,theta_new)
print(results)
return(results)
}
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
intercept=coef_m[1]
coef_x=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
results=gradDescent (y, x,intercept,coef_x,prev_yhat,alpha)
return(results)
}
coef_new=gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef_new)
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
results=c(intercept_new,theta_new)
#print(results)
return(results)
}
coef_new=gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef_new)
y_pred=predict_osl(train_titanic_3,coef_new)
print(y_pred)
y_pred=predict_osl(train_titanic_3,coef)
print(y_pred_old)
y_pred_old=predict_osl(train_titanic_3,coef)
print(y_pred_old)
y_pred=predict_osl(train_titanic_3,coef_new)
print(y_pred)
train_titanic_3=titanic[10:10,]
#determine the new coefficients
coef_new=gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef_new)
y_pred=predict_osl(train_titanic_3,coef_new)
print(y_pred)
#'Test of online algorith
#'start with glmspeed to determine coefficients
#'change to logit/expit plogis/qlogis in a continu value
#'use the coefficients as a starting point for gradient descent
#'save the coefficients and turn the out come to a logistic regression prediction
#'
#' read csv file (read.csv("csvfilename.csv"))
library(speedglm)
titanic=read.csv("train_titanic.csv")
#divide dataset in three subsets (subset(datasetname,factor <= or == or > or < value))
train_titanic_1=titanic[0:700,]
train_titanic_2=titanic[701:891,]
train_titanic_3=titanic[10:10,]
#generate the first model based on the speedglm package
glm_model<- speedglm(y~x1+x2,train_titanic_1,family = binomial(logit))
summary(glm_model)
#predict the result based on the model
glm_model_pred=predict(glm_model,newdata=train_titanic_3,type="response")
#determine the yhat with a threshold of 0.5
yhat <- ifelse(glm_model_pred > 0.5,1,0)
#determine the coefficients of the model
coef=(coef(glm_model))
# function to predict with the coefficients, it is a logit function so the final result is converted
predict_osl<-function(row,coef){
row_m=as.matrix(row)
coef_m=as.matrix(coef)
#determine number of features
n=ncol(row)-1
#determine the value of the intercept
yhat_intercept = coef_m[1]
y_pred=0
#calculate the prediction of y
#to do matrix, vectorize
#for (i in 1:n){
#  y_pred = y_pred + coef_m[i + 1] * row_m[i+1]
#}
#BROWSER
y_pred <- coef %*% t(cbind(intercept = 1, row[-1]))
#convert the result into a logit function
yhat_return=plogis(y_pred)
return(yhat_return)
}
#Gradient descent function
gradDescent<-function(y, x,theta,alpha){
#calculate the adjustment of the theta and intercept based on one record
#(coefficients are a combination of theta and intercept )
n <- length(y)
# this is a vectorized form for the gradient of the cost function
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
#browser()
results=theta_new
return(results)
}
gradientdescent_osl<-function (row,coef,alpha){
#prediction of the yhat using the former coefficients
#prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
#determine intercept
#intercept=coef_m[1]
#determine the coefficients (or theta)
#coef_x=coef_m[-1,]
#the values of the row
y=row_m[1]
x=row_m[,-1]
x_i=c(intercept=1,x)
#browser()
#prev_yat eruit
results=gradDescent (y, x_i,coef_m,alpha)
return(results)
}
coef=gradientdescent_osl(train_titanic_3,coef,0.001)
print(coef)
coef=gradientdescent_osl(train_titanic_3,coef,0.001)
print(coef)
coef=gradientdescent_osl(train_titanic_3,coef,0.001)
print(coef)
setwd("C:/Users/talko/OneDrive - Hanzehogeschool Groningen/R-sandbox/R-sandbox/TMLE Tutorial")
# The simulated data replicationg the DAG in Figure 1:
#  1. Y: mortality binary indicator (1 death, 0 alive)
#2. A: binary treatment (1 Chemotherapy, 0 Radiotherapy )
#3. W1: Gender (1 male; 0 female)
#4. W2: Age at diagnosis (0 <65; 1 >=65)
#5. W3: Cancer TNM classification (scale from 1 to 4; 1: early stage no metastasis; 4:
#                                    advanced stage with metastasis)
#6. W4: Comorbidities (scale from 1 to 5)
options(digits=4)
