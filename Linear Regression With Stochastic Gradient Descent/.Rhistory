coef_m=as.matrix(coef)
n=ncol(row)-1
yhat_intercept = coef_m[1]
#print(yhat_intercept)
y_pred=0
for (i in 1:n){
#print(row_m[i+1])
#print(coef_m[i+1])
y_pred = y_pred + coef_m[i + 1] * row_m[i+1]
#print(y_pred)
}
yhat_return=plogis(yhat_intercept+y_pred)
return(yhat_return)
}
glm_model<- speedglm(y~x1+x2,train_titanic_1,family = binomial(logit))
glm_model_pred=predict(glm_model,newdata=train_titanic_3,type="response")
predict_sgd<-function(row,coef){
row_m=as.matrix(row)
#print(row_m[3])
#print(row_m)
coef_m=as.matrix(coef)
n=ncol(row)-1
yhat_intercept = coef_m[1]
#print(yhat_intercept)
y_pred=0
for (i in 1:n){
#print(row_m[i+1])
#print(coef_m[i+1])
y_pred = y_pred + coef_m[i + 1] * row_m[i+1]
#print(y_pred)
}
yhat_return=plogis(yhat_intercept+y_pred)
return(yhat_return)
}
y_pred=predict_sgd(train_titanic_3,coef)
print(y_pred)
train_titanic_3=titanic[890:890,]
train_titanic_3=titanic[890:890,]
y_pred=predict_sgd(train_titanic_3,coef)
print(y_pred)
glm_model_pred=predict(glm_model,newdata=train_titanic_3,type="response")
train_titanic_3=titanic[889:889,]
# function to predict with the coefficients
predict_sgd<-function(row,coef){
row_m=as.matrix(row)
#print(row_m[3])
#print(row_m)
coef_m=as.matrix(coef)
n=ncol(row)-1
yhat_intercept = coef_m[1]
#print(yhat_intercept)
y_pred=0
for (i in 1:n){
#print(row_m[i+1])
#print(coef_m[i+1])
y_pred = y_pred + coef_m[i + 1] * row_m[i+1]
#print(y_pred)
}
#convert the result into a logit function
yhat_return=plogis(yhat_intercept+y_pred)
return(yhat_return)
}
y_pred=predict_sgd(train_titanic_3,coef)
print(y_pred)
glm_model_pred=predict(glm_model,newdata=train_titanic_3,type="response")
gradientR<-function(y, X, epsilon,eta, iters){
epsilon = 0.0001
X = as.matrix(data.frame(rep(1,length(y)),X))
N= dim(X)[1]
print("Initialize parameters...")
theta.init = as.matrix(rnorm(n=dim(X)[2], mean=0,sd = 1)) # Initialize theta
print(theta.init)
theta.init = t(theta.init)
e = t(y) - theta.init%*%t(X)
grad.init = -(2/N)%*%(e)%*%X
theta = theta.init - eta*(1/N)*grad.init
l2loss = c()
for(i in 1:iters){
l2loss = c(l2loss,sqrt(sum((t(y) - theta%*%t(X))^2)))
e = t(y) - theta%*%t(X)
grad = -(2/N)%*%e%*%X
theta = theta - eta*(2/N)*grad
print(sqrt(sum(grad^2)))
if(sqrt(sum(grad^2)) <= epsilon){
break
}
}
print("Algorithm converged")
print(paste("Final gradient norm is",sqrt(sum(grad^2))))
values<-list("coef" = t(theta), "l2loss" = l2loss)
return(values)
}
gdec.eta1 = gradientR(y = y, X = data.frame(x1,x2,x3, x4,x5), eta = 100, iters = 1000)
gradientDesc <- function(x, y, learn_rate, conv_threshold, n, max_iter) {
plot(x, y, col = "blue", pch = 20)
m <- runif(1, 0, 1)
c <- runif(1, 0, 1)
yhat <- m * x + c
MSE <- sum((y - yhat) ^ 2) / n
converged = F
iterations = 0
while(converged == F) {
## Implement the gradient descent algorithm
m_new <- m - learn_rate * ((1 / n) * (sum((yhat - y) * x)))
c_new <- c - learn_rate * ((1 / n) * (sum(yhat - y)))
m <- m_new
c <- c_new
yhat <- m * x + c
MSE_new <- sum((y - yhat) ^ 2) / n
if(MSE - MSE_new <= conv_threshold) {
abline(c, m)
converged = T
return(paste("Optimal intercept:", c, "Optimal slope:", m))
}
iterations = iterations + 1
if(iterations > max_iter) {
abline(c, m)
converged = T
return(paste("Optimal intercept:", c, "Optimal slope:", m))
}
}
}
# Run the function
gradientDesc(disp, mpg, 0.0000293, 0.001, 32, 2500000)
attach(mtcars)
gradientDesc(disp, mpg, 0.0000293, 0.001, 32, 2500000)
row_m
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*(t(x)%*%(x%*%theta - y))
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
# for a R function to return two values, we need to use a list to store them:
results<-list(intercept_new,theta_new)
return(results)
}
gradientdescent_osl<-function (row,coef,learning_rate){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[-1,]
intercept=coef_m[1]
gradDescent (y, x,intercept,coef,prev_yat,0.0001)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
predict_osl<-function(row,coef){
row_m=as.matrix(row)
#print(row_m[3])
#print(row_m)
coef_m=as.matrix(coef)
n=ncol(row)-1
yhat_intercept = coef_m[1]
#print(yhat_intercept)
y_pred=0
for (i in 1:n){
#print(row_m[i+1])
#print(coef_m[i+1])
y_pred = y_pred + coef_m[i + 1] * row_m[i+1]
#print(y_pred)
}
#convert the result into a logit function
yhat_return=plogis(yhat_intercept+y_pred)
return(yhat_return)
}
y_pred=predict_osl(train_titanic_3,coef)
print(y_pred)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[-1,]
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef,prev_yhat,alpha)
print(result)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef)
glm_model_pred
glm_model<- speedglm(y~x1+x2,train_titanic_1,family = binomial(logit))
summary(glm_model)
glm_model_pred=predict(glm_model,newdata=train_titanic_3,type="response")
yhat <- ifelse(glm_model_pred > 0.5,1,0)
coef=(coef(glm_model))
y_pred=predict_osl(train_titanic_3,coef)
print(y_pred)
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[-1,]
print(x)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[-1,]
print(y)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[-2,]
print(x)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[,-1]
print(x)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[,-1]
print(x)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
y=row_m[1]
x=row_m[,-1]
print(x)
print(coef_m)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
coef_m=coef_m[,-1]
y=row_m[1]
x=row_m[,-1]
print(x)
print(coef_m)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
coef_m=coef_m[-1,]
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
coef_m=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
print(x)
print(coef_m)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
# for a R function to return two values, we need to use a list to store them:
results<-list(intercept_new,theta_new)
return(results)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
coef_m=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
print(x)
print(coef_m)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
coef_m=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
print(x)
print(coef_m)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
print(result)
print(coef)
}
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
print(intercept)
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
print(intercept_new)
# for a R function to return two values, we need to use a list to store them:
results<-list(intercept_new,theta_new)
return(results)
}
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
coef_m=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
print(x)
print(coef_m)
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
coef_m=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
intercept=coef_m[1]
result=gradDescent (y, x,intercept,coef_m,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
intercept=coef_m[1]
coef_x=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
result=gradDescent (y, x,intercept,coef_x,prev_yhat,alpha)
#print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
# for a R function to return two values, we need to use a list to store them:
results<-list(intercept_new,theta_new)
return(results)
}
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
intercept=coef_m[1]
coef_x=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
result=gradDescent (y, x,intercept,coef_x,prev_yhat,alpha)
print(result)
#print(coef)
}
gradientdescent_osl(train_titanic_3,coef,0.0001)
coef_new=gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef_new)
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
results_c=c(intercept_new,theta_new)
print(results_c)
# for a R function to return two values, we need to use a list to store them:
results<-list(intercept_new,theta_new)
return(results)
}
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
intercept=coef_m[1]
coef_x=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
result=gradDescent (y, x,intercept,coef_x,prev_yhat,alpha)
#print(result)
#print(coef)
}
coef_new=gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef_new)
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
intercept=coef_m[1]
coef_x=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
results=gradDescent (y, x,intercept,coef_x,prev_yhat,alpha)
return(results)
}
coef_new=gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef_new)
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
results=c(intercept_new,theta_new)
print(results)
return(results)
}
gradientdescent_osl<-function (row,coef,alpha){
prev_yhat=predict_osl(row,coef)
row_m=as.matrix(row)
coef_m=as.matrix(coef)
intercept=coef_m[1]
coef_x=coef_m[-1,]
y=row_m[1]
x=row_m[,-1]
results=gradDescent (y, x,intercept,coef_x,prev_yhat,alpha)
return(results)
}
coef_new=gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef_new)
gradDescent<-function(y, x,intercept,theta,yhat,alpha){
n <- length(y)
# this is a vectorized form for the gradient of the cost function
# X is a  matrix, theta is a  column vector, y is a 100x1 column vector
# X transpose is a 5x100 matrix. So t(X)%*%(X%*%theta - y) is a 5x1 column vector
theta_new <- theta - alpha*(1/n)*((x)%*%(x%*%theta - y))
intercept_new <- intercept - alpha * ((1 / n) * (yhat - y))
results=c(intercept_new,theta_new)
#print(results)
return(results)
}
coef_new=gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef_new)
y_pred=predict_osl(train_titanic_3,coef_new)
print(y_pred)
y_pred=predict_osl(train_titanic_3,coef)
print(y_pred_old)
y_pred_old=predict_osl(train_titanic_3,coef)
print(y_pred_old)
y_pred=predict_osl(train_titanic_3,coef_new)
print(y_pred)
train_titanic_3=titanic[10:10,]
#determine the new coefficients
coef_new=gradientdescent_osl(train_titanic_3,coef,0.0001)
print(coef_new)
y_pred=predict_osl(train_titanic_3,coef_new)
print(y_pred)
